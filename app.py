import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from dotenv import load_dotenv
import os

# --- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()

# --- Streamlit UI ---
st.title("ğŸ’¬ å°‚é–€å®¶ã«è³ªå•ã§ãã‚‹AIã‚¢ãƒ—ãƒª")
st.write("ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã‚’é¸ã³ã€è³ªå•ã‚’å…¥åŠ›ã—ã¦AIã«èã„ã¦ã¿ã‚ˆã†ï¼")

# --- å°‚é–€å®¶ã®é¸æŠ ---
role = st.radio(
    "å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸ã‚“ã§ãã ã•ã„ï¼š",
    ["å¿ƒç†å­¦è€…", "æ–™ç†ç ”ç©¶å®¶", "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢", "æ­´å²å­¦è€…"]
)

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ---
question = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

# --- LLMã«è³ªå•ã™ã‚‹é–¢æ•° ---
def ask_expert(role, question):
    system_prompt = f"ã‚ãªãŸã¯{role}ã§ã™ã€‚å°‚é–€å®¶ã®è¦–ç‚¹ã‹ã‚‰ã‚ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚"
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=question)
    ]
    response = llm.invoke(messages)
    return response.content

# --- ãƒœã‚¿ãƒ³æŠ¼ä¸‹ã§å®Ÿè¡Œ ---
if st.button("é€ä¿¡") and question:
    with st.spinner("AIãŒè€ƒãˆä¸­ã§ã™..."):
        answer = ask_expert(role, question)
        st.success("å›ç­”ï¼š")
        st.write(answer)
